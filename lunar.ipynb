{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gymhelpers import ExperimentsManager\n",
    "import scipy.io as scipyio\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: LunarLanderContinuous-v2, Actions: 961, Temp: 0.01, Strategy: epsilon, Backup: bellman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-31 19:01:05,479] Making new env: LunarLanderContinuous-v2\n",
      "[2017-07-31 19:01:05,495] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTING EXPERIMENT 0 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n",
      "Final mean reward, averaged over 1 experiment: -633.818483492 (std = 0.0).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "plot_utils.py:83: RuntimeWarning: invalid value encountered in divide\n",
      "  factor = higher_border / mu\n",
      "[2017-07-31 19:42:03,280] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode duration: 613.885409 ms\n",
      "\n",
      "EXECUTING EXPERIMENT 1 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n",
      "Final mean reward, averaged over 2 experiments: -559.074481364 (std = 74.7440021281).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-31 20:53:44,485] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode duration: 1074.729788 ms\n",
      "\n",
      "EXECUTING EXPERIMENT 2 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n",
      "Final mean reward, averaged over 3 experiments: -594.379488972 (std = 78.8500540726).\n",
      "Average episode duration: 737.497257 ms\n",
      "Average final reward: -590.46 (std=246.74).\n",
      "\n",
      "The 100-episode moving average reached 200 after 0 episodes.\n",
      "Problem: LunarLanderContinuous-v2, Actions: 961, Temp: 0.01, Strategy: epsilon, Backup: sparsebellman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-31 21:43:00,746] Making new env: LunarLanderContinuous-v2\n",
      "[2017-07-31 21:43:00,751] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTING EXPERIMENT 0 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n",
      "Final mean reward, averaged over 1 experiment: -477.73662309 (std = 0.0).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-31 22:26:20,136] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode duration: 649.281170 ms\n",
      "\n",
      "EXECUTING EXPERIMENT 1 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n",
      "Final mean reward, averaged over 2 experiments: -518.124396063 (std = 40.387772973).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-31 23:09:39,800] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode duration: 649.361486 ms\n",
      "\n",
      "EXECUTING EXPERIMENT 2 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n",
      "Final mean reward, averaged over 3 experiments: -557.059398135 (std = 64.1819048473).\n",
      "Average episode duration: 630.546128 ms\n",
      "Average final reward: -554.10 (std=160.71).\n",
      "\n",
      "The 100-episode moving average reached 200 after 0 episodes.\n",
      "Problem: LunarLanderContinuous-v2, Actions: 961, Temp: 0.01, Strategy: epsilon, Backup: softbellman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-31 23:51:48,146] Making new env: LunarLanderContinuous-v2\n",
      "[2017-07-31 23:51:48,150] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTING EXPERIMENT 0 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n",
      "Final mean reward, averaged over 1 experiment: -565.940974225 (std = 0.0).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-01 00:37:46,066] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode duration: 688.912735 ms\n",
      "\n",
      "EXECUTING EXPERIMENT 1 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n",
      "Final mean reward, averaged over 2 experiments: -576.200585172 (std = 10.259610947).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-01 01:19:53,915] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode duration: 631.418615 ms\n",
      "\n",
      "EXECUTING EXPERIMENT 2 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n",
      "Final mean reward, averaged over 3 experiments: -579.450979406 (std = 9.55527096477).\n",
      "Average episode duration: 816.729990 ms\n",
      "Average final reward: -578.89 (std=164.08).\n",
      "\n",
      "The 100-episode moving average reached 200 after 0 episodes.\n",
      "Problem: LunarLanderContinuous-v2, Actions: 961, Temp: 0.01, Strategy: sparsemax, Backup: bellman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-01 02:14:27,078] Making new env: LunarLanderContinuous-v2\n",
      "[2017-08-01 02:14:27,083] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTING EXPERIMENT 0 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "maxapproxi.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p/p.sum()\n",
      "agents.py:112: RuntimeWarning: invalid value encountered in less\n",
      "  a = np.random.choice(self.n_actions, p=policy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mean reward, averaged over 1 experiment: -976.126264899 (std = 0.0).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-01 05:53:43,503] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode duration: 3288.561200 ms\n",
      "\n",
      "EXECUTING EXPERIMENT 1 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n",
      "Final mean reward, averaged over 2 experiments: -631.627077441 (std = 344.499187457).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-01 10:58:14,411] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode duration: 4567.169013 ms\n",
      "\n",
      "EXECUTING EXPERIMENT 2 OF 3 IN ENVIRONMENT LunarLanderContinuous-v2.\n"
     ]
    }
   ],
   "source": [
    "strategies = [\"epsilon\",\"sparsemax\",\"softmax\"]\n",
    "backuprules = [\"bellman\",\"sparsebellman\",\"softbellman\"]\n",
    "temperatures = [0.01,0.1,1]\n",
    "temperatures_name = [\"low\",\"mid\",\"high\"]\n",
    "action_res_list = [[31, 31], [51, 51], [11, 11], [3,3]]\n",
    "action_res_name = [\"midlarge\",\"large\",\"midsmall\",\"small\"]\n",
    "\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "min_avg_rwd = 200\n",
    "stop_training_min_avg_rwd = 210\n",
    "layers_size = [512, 512]\n",
    "n_ep = 4000\n",
    "n_exps = 3\n",
    "\n",
    "gym_stats_dir_prefix = os.path.join('Gym_stats', env_name)\n",
    "figures_dir = 'Figures'\n",
    "api_key = '###'\n",
    "alg_id = '###'\n",
    "\n",
    "data = defaultdict(lambda : defaultdict(lambda : defaultdict(lambda : defaultdict(lambda : None))))\n",
    "for action_res, action_name in zip(action_res_list,action_res_name):\n",
    "    for temperature, temperature_name in zip(temperatures, temperatures_name):\n",
    "        for strategy in strategies:\n",
    "            for backuprule in backuprules:\n",
    "                print(\"Problem: {}, Actions: {}, Temp: {}, Strategy: {}, Backup: {}\".format(env_name,np.prod(action_res),temperature,strategy,backuprule))\n",
    "                expsman = ExperimentsManager(env_name=env_name, agent_value_function_hidden_layers_size=layers_size,\n",
    "                                      figures_dir=figures_dir, discount=0.99, decay_eps=0.999, eps_min=1E-4, learning_rate=3E-4,\n",
    "                                      decay_lr=True, max_step=10000, replay_memory_max_size=10000, ep_verbose=False,\n",
    "                                      exp_verbose=False, learning_rate_end=3E-5, batch_size=128, upload_last_exp=False, double_dqn=True, dueling=False,\n",
    "                                      target_params_update_period_steps=50, replay_period_steps=4, min_avg_rwd=min_avg_rwd,\n",
    "                                      per_proportional_prioritization=True, per_apply_importance_sampling=True, per_alpha=0.2,\n",
    "                                      per_beta0=0.4,\n",
    "                                      results_dir_prefix=gym_stats_dir_prefix, gym_api_key=api_key, gym_algorithm_id=alg_id,\n",
    "                                      strategy=strategy,backuprule=backuprule,temperature=temperature,action_res=action_res)\n",
    "                _, _, Rwd_per_ep_v, Loss_per_ep_v = expsman.run_experiments(n_exps=n_exps, n_ep=n_ep, stop_training_min_avg_rwd=stop_training_min_avg_rwd, plot_results=False)\n",
    "                data[action_name][temperature_name][strategy][backuprule] = {\"reward_list\":Rwd_per_ep_v,\"loss_list\":Loss_per_ep_v}\n",
    "\n",
    "scipyio.savemat(env_name+\".mat\", data)\n",
    "print(\"{} is finished and is saved\".format(env_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
